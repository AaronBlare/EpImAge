{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Debugging autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import numpy as np\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig, GANDALFConfig, TabNetModelConfig, FTTransformerConfig, DANetConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "import pandas as pd\n",
    "from src.utils.configs import read_parse_config\n",
    "from src.pt.hyper_opt import train_hyper_opt\n",
    "import optuna\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "path_root = pathlib.Path(os.getcwd())\n",
    "path_data = f\"{path_root}/data/immuno-regression\"\n",
    "df_feats = pd.read_excel(f\"{path_data}/features.xlsx\")\n",
    "imms = df_feats.columns.to_list()\n",
    "df = pd.read_excel(f\"{path_data}/data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_n_splits = 5\n",
    "tst_n_repeats = 5\n",
    "tst_random_state = seed\n",
    "\n",
    "val_n_splits = 4\n",
    "val_n_repeats = 4\n",
    "val_random_state = seed\n",
    "\n",
    "stratify_cat_parts_all = {\n",
    "    'Control': df.index[df['Status'] == 'Control'].values,\n",
    "    'ESRD': df.index[df['Status'] == 'ESRD'].values,\n",
    "}\n",
    "\n",
    "for part_all, ids_all in stratify_cat_parts_all.items():\n",
    "    trgt_all = df.loc[ids_all, 'Age'].values\n",
    "    ptp_all = np.ptp(trgt_all)\n",
    "    num_bins_all = 5\n",
    "    bins_all = np.linspace(np.min(trgt_all) - 0.1 * ptp_all, np.max(trgt_all) + 0.1 * ptp_all, num_bins_all + 1)\n",
    "    binned_all = np.digitize(trgt_all, bins_all) - 1\n",
    "    unique_all, counts_tst = np.unique(binned_all, return_counts=True)\n",
    "    \n",
    "    k_fold_all = RepeatedStratifiedKFold(\n",
    "        n_splits=tst_n_splits,\n",
    "        n_repeats=tst_n_repeats,\n",
    "        random_state=tst_random_state\n",
    "    )\n",
    "    splits_all = k_fold_all.split(X=ids_all, y=binned_all, groups=binned_all)\n",
    "    \n",
    "    for split_id, (ids_trn_val, ids_tst) in enumerate(splits_all):\n",
    "        df.loc[ids_all[ids_trn_val], f\"Split_{split_id}\"] = \"trn_val\"\n",
    "        df.loc[ids_all[ids_tst], f\"Split_{split_id}\"] = \"tst\"\n",
    "\n",
    "samples = {}\n",
    "for split_id in range(tst_n_splits * tst_n_repeats):\n",
    "    samples[split_id] = {\n",
    "        'test': df.index[df[f\"Split_{split_id}\"] == \"tst\"].values,\n",
    "        'train_validation': df.index[df[f\"Split_{split_id}\"] == \"trn_val\"].values,\n",
    "        'trains': {},\n",
    "        'validations': {},\n",
    "    }\n",
    "\n",
    "    stratify_cat_parts_trnval = {\n",
    "        'Control': df.index[(df['Status'] == 'Control') & (df[f\"Split_{split_id}\"] == 'trn_val')].values,\n",
    "        'ESRD': df.index[(df['Status'] == 'ESRD') & (df[f\"Split_{split_id}\"] == 'trn_val')].values,\n",
    "    }\n",
    "\n",
    "    for part_trnval, ids_trnval in stratify_cat_parts_trnval.items():\n",
    "        trgt_trnval = df.loc[ids_trnval, 'Age'].values\n",
    "        ptp_trnval = np.ptp(trgt_trnval)\n",
    "        num_bins_trnval = 5\n",
    "        bins_trnval = np.linspace(np.min(trgt_trnval) - 0.1 * ptp_trnval, np.max(trgt_trnval) + 0.1 * ptp_trnval, num_bins_trnval + 1)\n",
    "        binned_trnval = np.digitize(trgt_trnval, bins_trnval) - 1\n",
    "        unique_trnval, counts_trnval = np.unique(binned_trnval, return_counts=True)\n",
    "        k_fold_trnval = RepeatedStratifiedKFold(\n",
    "            n_splits=val_n_splits,\n",
    "            n_repeats=val_n_repeats,\n",
    "            random_state=val_random_state\n",
    "        )\n",
    "        splits_trnval = k_fold_trnval.split(X=ids_trnval, y=binned_trnval, groups=binned_trnval)\n",
    "        \n",
    "        for fold_id, (ids_trn, ids_val) in enumerate(splits_trnval):\n",
    "            df.loc[ids_trnval[ids_trn], f\"Split_{split_id}_Fold_{fold_id}\"] = \"trn\"\n",
    "            df.loc[ids_trnval[ids_val], f\"Split_{split_id}_Fold_{fold_id}\"] = \"val\"\n",
    "         \n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        samples[split_id]['trains'][fold_id] = df.index[df[f\"Split_{split_id}_Fold_{fold_id}\"] == \"trn\"].values\n",
    "        samples[split_id]['validations'][fold_id] = df.index[df[f\"Split_{split_id}_Fold_{fold_id}\"] == \"val\"].values\n",
    "\n",
    "    samples[split_id]['cv_indexes'] = [\n",
    "        (\n",
    "            np.where(df.index[df[f\"Split_{split_id}\"] == \"trn_val\"].isin(df.index[(df[f\"Split_{split_id}\"] == \"trn_val\") & (df[f\"Split_{split_id}_Fold_{i}\"] == 'trn')]))[0],\n",
    "            np.where(df.index[df[f\"Split_{split_id}\"] == \"trn_val\"].isin(df.index[(df[f\"Split_{split_id}\"] == \"trn_val\") & (df[f\"Split_{split_id}_Fold_{i}\"] == 'val')]))[0],\n",
    "        )\n",
    "        for i in range(val_n_splits * val_n_repeats)\n",
    "    ]\n",
    "    \n",
    "# Chekning for non-intersection\n",
    "for split_id in range(tst_n_splits * tst_n_repeats):\n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        test_samples = samples[split_id]['test']\n",
    "        train_samples = samples[split_id]['trains'][fold_id]\n",
    "        validation_samples = samples[split_id]['validations'][fold_id]\n",
    "\n",
    "        intxns = {\n",
    "            'train_validation': set.intersection(set(train_samples), set(validation_samples)),\n",
    "            'validation_test': set.intersection(set(validation_samples), set(test_samples)),\n",
    "            'train_test': set.intersection(set(train_samples), set(test_samples))\n",
    "        }\n",
    "        \n",
    "        for intxn_name, intxn_samples in intxns.items():\n",
    "            if len(intxn_samples) > 0:\n",
    "                raise ValueError(f\"Non-zero {intxn_name} intersection ({len(intxn_samples)}) for {split_id} Split and {fold_id} Fold!\")\n",
    "\n",
    "with open(f\"{path_data}/stratification.pickle\", 'wb') as handle:\n",
    "    pickle.dump(samples, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{path_data}/stratification.pickle\", 'rb') as handle:\n",
    "    samples = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PytorchTabular configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data, Trainer, Optimizer configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_configs = f\"{path_root}/configs/immuno-regression\"\n",
    "\n",
    "data_config = read_parse_config(f\"{path_configs}/DataConfig.yaml\", DataConfig)\n",
    "trainer_config = read_parse_config(f\"{path_configs}/TrainerConfig.yaml\", TrainerConfig)\n",
    "trainer_config['seed'] = seed\n",
    "trainer_config['checkpoints'] = 'valid_loss'\n",
    "trainer_config['load_best'] = True\n",
    "trainer_config['auto_lr_find'] = False\n",
    "optimizer_config = read_parse_config(f\"{path_configs}/OptimizerConfig.yaml\", OptimizerConfig)\n",
    "\n",
    "lr_find_min_lr = 1e-8\n",
    "lr_find_max_lr = 10\n",
    "lr_find_num_training = 512\n",
    "lr_find_mode = \"exponential\"\n",
    "lr_find_early_stop_threshold = 8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load default Models configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_archs = ['DANet', 'FTTransformer', 'GANDALF', 'TabNetModel' , 'CategoryEmbeddingModel']\n",
    "configs_models_default = {}\n",
    "configs_models_default['DANet'] = read_parse_config(f\"{path_configs}/models/DANetConfig.yaml\", DANetConfig)\n",
    "configs_models_default['FTTransformer'] = read_parse_config(f\"{path_configs}/models/FTTransformerConfig.yaml\", FTTransformerConfig)\n",
    "configs_models_default['GANDALF'] = read_parse_config(f\"{path_configs}/models/GANDALFConfig.yaml\", GANDALFConfig)\n",
    "configs_models_default['TabNetModel'] = read_parse_config(f\"{path_configs}/models/TabNetModelConfig.yaml\", TabNetModelConfig)\n",
    "configs_models_default['CategoryEmbeddingModel'] = read_parse_config(f\"{path_configs}/models/CategoryEmbeddingModelConfig.yaml\", CategoryEmbeddingModelConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training immunomarkers models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPE sampler\n",
    "n_trials = 5 # 512\n",
    "opt_seed = seed\n",
    "n_startup_trials = 3 # 128\n",
    "n_ei_candidates = 2 # 16\n",
    "\n",
    "# Init optimization metrics with directions\n",
    "opt_parts = ['test', 'validation']\n",
    "opt_metrics = [('mean_absolute_error', 'minimize'), ('pearson_corrcoef', 'maximize')]\n",
    "opt_directions = []\n",
    "for part in opt_parts:\n",
    "    for metric_pair in opt_metrics:\n",
    "        opt_directions.append(f\"{metric_pair[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation and Hyperparameter optimization training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for immunomarkers\n",
    "for imm in imms:\n",
    "    \n",
    "    feats = df_feats[imm].to_list()\n",
    "    data_config['target'] = [imm]\n",
    "    data_config['continuous_cols'] = feats\n",
    "    \n",
    "    path_ckpts = f\"{path_root}/logs/Immunomarkers/{imm}\"\n",
    "    pathlib.Path(path_ckpts).mkdir(parents=True, exist_ok=True)\n",
    "    trainer_config['checkpoints_path'] = path_ckpts\n",
    "    \n",
    "    # Dataframes with results\n",
    "    dfs_results = []\n",
    "    \n",
    "    # Loop for train-validation / test splits\n",
    "    for split_id, split_dict in samples.items():\n",
    "        # Loop for trair/validation folds\n",
    "        for fold_id in split_dict['trains']:\n",
    "            \n",
    "            test = df.loc[split_dict['test'], feats + [imm]]\n",
    "            train = df.loc[split_dict['trains'][fold_id], feats + [imm]]\n",
    "            validation = df.loc[split_dict['validations'][fold_id], feats + [imm]]\n",
    "            \n",
    "            # Loop for models archs\n",
    "            for m_arch in models_archs:\n",
    "                \n",
    "                tabular_model_default = TabularModel(\n",
    "                    data_config=data_config,\n",
    "                    model_config=configs_models_default[m_arch],\n",
    "                    optimizer_config=optimizer_config,\n",
    "                    trainer_config=trainer_config,\n",
    "                    verbose=False,\n",
    "                )\n",
    "                datamodule = tabular_model_default.prepare_dataloader(train=train, validation=validation, seed=seed)\n",
    "                \n",
    "                trials_results = []\n",
    "                study = optuna.create_study(\n",
    "                    study_name=f\"{imm}_{split_id}_{fold_id}_{m_arch}\",\n",
    "                    sampler=optuna.samplers.TPESampler(\n",
    "                        n_startup_trials=n_startup_trials,\n",
    "                        n_ei_candidates=n_ei_candidates,\n",
    "                        seed=opt_seed,\n",
    "                    ),\n",
    "                    directions=opt_directions\n",
    "                )\n",
    "                study.optimize(\n",
    "                    func=lambda trial: train_hyper_opt(\n",
    "                        trial=trial,\n",
    "                        trials_results=trials_results,\n",
    "                        opt_metrics=opt_metrics,\n",
    "                        opt_parts=opt_parts,\n",
    "                        model_config_default=configs_models_default[m_arch],\n",
    "                        data_config_default=data_config,\n",
    "                        optimizer_config_default=optimizer_config,\n",
    "                        trainer_config_default=trainer_config,\n",
    "                        experiment_config_default=None,\n",
    "                        train=train,\n",
    "                        validation=validation,\n",
    "                        test=test,\n",
    "                        datamodule=datamodule,\n",
    "                        min_lr=lr_find_min_lr,\n",
    "                        max_lr=lr_find_max_lr,\n",
    "                        num_training=lr_find_num_training,\n",
    "                        mode=lr_find_mode,\n",
    "                        early_stop_threshold=lr_find_early_stop_threshold\n",
    "                    ), \n",
    "                    n_trials=n_trials, \n",
    "                    show_progress_bar=False\n",
    "                )\n",
    "                df_trials = pd.DataFrame(trials_results)\n",
    "                df_trials['split_id'] = split_id\n",
    "                df_trials['fold_id'] = fold_id\n",
    "                dfs_results.append(df_trials)\n",
    "    \n",
    "    # Resulting Dataframe for immunomarker\n",
    "    df_results = pd.concat(dfs_results)            \n",
    "    df_results.sort_values(by=['test_loss'], ascending=[True], inplace=True)\n",
    "    df_results.to_excel(f\"{path_ckpts}/results.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
