{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Debugging autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T14:29:22.801727Z",
     "start_time": "2024-07-01T14:29:18.695223Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import numpy as np\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig, GANDALFConfig, TabNetModelConfig, FTTransformerConfig, DANetConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "import pandas as pd\n",
    "from src.utils.configs import read_parse_config\n",
    "from src.pt.hyper_opt import train_hyper_opt\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "path_root = pathlib.Path(os.getcwd())\n",
    "path_plots = f\"{path_root}/plots\"\n",
    "path_data = f\"{path_root}/data/age-regression\"\n",
    "df_feats = pd.read_excel(f\"{path_data}/features.xlsx\", index_col=0)\n",
    "imms = df_feats.index.to_list()\n",
    "df = pd.read_excel(f\"{path_data}/data.xlsx\")\n",
    "df_controls = df[df['Status'] == 'Control']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_n_splits = 5\n",
    "tst_n_repeats = 5\n",
    "tst_random_state = seed\n",
    "\n",
    "val_n_splits = 4\n",
    "val_n_repeats = 4\n",
    "val_random_state = seed\n",
    "\n",
    "quantiles = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "gse_count = df_controls['GSE'].value_counts()\n",
    "stratify_cat_parts_all = {gse: df_controls.index[df_controls['GSE'] == gse].values for gse, count in gse_count.items()}\n",
    "\n",
    "for part_all, ids_all in (pbar := tqdm(stratify_cat_parts_all.items())):\n",
    "    pbar.set_description(f\"Processing {part_all} ({len(ids_all)})\")\n",
    "    quantiles_all = pd.qcut(df_controls.loc[ids_all, 'Age'].values, quantiles, labels=False, duplicates='drop')\n",
    "    unique_all, counts_all = np.unique(quantiles_all, return_counts=True)\n",
    "    \n",
    "    if max(counts_all) >= len(quantiles):\n",
    "        k_fold_all = RepeatedStratifiedKFold(\n",
    "            n_splits=tst_n_splits,\n",
    "            n_repeats=tst_n_repeats,\n",
    "            random_state=tst_random_state\n",
    "        )\n",
    "        splits_all = k_fold_all.split(X=ids_all, y=quantiles_all, groups=quantiles_all)\n",
    "        for split_id, (ids_trn_val, ids_tst) in enumerate(splits_all):\n",
    "            df_controls.loc[ids_all[ids_trn_val], f\"Split_{split_id}\"] = \"trn_val\"\n",
    "            df_controls.loc[ids_all[ids_tst], f\"Split_{split_id}\"] = \"tst\"\n",
    "            \n",
    "    else:\n",
    "        for split_id in range(tst_n_splits * tst_n_repeats):\n",
    "            df_controls.loc[ids_all, f\"Split_{split_id}\"] = \"tst\"\n",
    "\n",
    "samples = {}\n",
    "for split_id in range(tst_n_splits * tst_n_repeats):\n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        df_controls[f\"Split_{split_id}_Fold_{fold_id}\"] = df_controls[f\"Split_{split_id}\"]\n",
    "    \n",
    "    samples[split_id] = {\n",
    "        'test': df_controls.index[df_controls[f\"Split_{split_id}\"] == \"tst\"].values,\n",
    "        'train_validation': df_controls.index[df_controls[f\"Split_{split_id}\"] == \"trn_val\"].values,\n",
    "        'trains': {},\n",
    "        'validations': {},\n",
    "    }\n",
    "    \n",
    "    stratify_cat_parts_trnval = {}\n",
    "    for gse, count in gse_count.items():\n",
    "        gse_ids = df_controls.index[(df_controls['GSE'] == gse) & (df_controls[f\"Split_{split_id}\"] == 'trn_val')].values\n",
    "        if len(gse_ids) > 0:\n",
    "            stratify_cat_parts_trnval[gse] = gse_ids\n",
    "\n",
    "    for part_trnval, ids_trnval in stratify_cat_parts_trnval.items():\n",
    "        quantiles_trnval = pd.qcut(df_controls.loc[ids_trnval, 'Age'].values, quantiles, labels=False, duplicates='drop')\n",
    "        unique_trnval, counts_trnval = np.unique(quantiles_trnval, return_counts=True)\n",
    "        k_fold_trnval = RepeatedStratifiedKFold(\n",
    "            n_splits=val_n_splits,\n",
    "            n_repeats=val_n_repeats,\n",
    "            random_state=val_random_state\n",
    "        )\n",
    "        splits_trnval = k_fold_trnval.split(X=ids_trnval, y=quantiles_trnval, groups=quantiles_trnval)\n",
    "        for fold_id, (ids_trn, ids_val) in enumerate(splits_trnval):\n",
    "            df_controls.loc[ids_trnval[ids_trn], f\"Split_{split_id}_Fold_{fold_id}\"] = \"trn\"\n",
    "            df_controls.loc[ids_trnval[ids_val], f\"Split_{split_id}_Fold_{fold_id}\"] = \"val\"\n",
    "         \n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        samples[split_id]['trains'][fold_id] = df_controls.index[df_controls[f\"Split_{split_id}_Fold_{fold_id}\"] == \"trn\"].values\n",
    "        samples[split_id]['validations'][fold_id] = df_controls.index[df_controls[f\"Split_{split_id}_Fold_{fold_id}\"] == \"val\"].values\n",
    "\n",
    "    samples[split_id]['cv_indexes'] = [\n",
    "        (\n",
    "            np.where(df_controls.index[df_controls[f\"Split_{split_id}\"] == \"trn_val\"].isin(df_controls.index[(df_controls[f\"Split_{split_id}\"] == \"trn_val\") & (df_controls[f\"Split_{split_id}_Fold_{i}\"] == 'trn')]))[0],\n",
    "            np.where(df_controls.index[df_controls[f\"Split_{split_id}\"] == \"trn_val\"].isin(df_controls.index[(df_controls[f\"Split_{split_id}\"] == \"trn_val\") & (df_controls[f\"Split_{split_id}_Fold_{i}\"] == 'val')]))[0],\n",
    "        )\n",
    "        for i in range(val_n_splits * val_n_repeats)\n",
    "    ]\n",
    "    \n",
    "# Chekning for non-intersection\n",
    "for split_id in range(tst_n_splits * tst_n_repeats):\n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        test_samples = samples[split_id]['test']\n",
    "        train_samples = samples[split_id]['trains'][fold_id]\n",
    "        validation_samples = samples[split_id]['validations'][fold_id]\n",
    "\n",
    "        intxns = {\n",
    "            'train_validation': set.intersection(set(train_samples), set(validation_samples)),\n",
    "            'validation_test': set.intersection(set(validation_samples), set(test_samples)),\n",
    "            'train_test': set.intersection(set(train_samples), set(test_samples))\n",
    "        }\n",
    "        \n",
    "        for intxn_name, intxn_samples in intxns.items():\n",
    "            if len(intxn_samples) > 0:\n",
    "                print(f\"Non-zero {intxn_name} intersection ({len(intxn_samples)}) for {split_id} Split and {fold_id} Fold!\")\n",
    "\n",
    "with open(f\"{path_data}/stratification.pickle\", 'wb') as handle:\n",
    "    pickle.dump(samples, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{path_data}/stratification.pickle\", 'rb') as handle:\n",
    "    samples = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PytorchTabular configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data, Trainer, Optimizer configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_configs = f\"{path_root}/configs/age-regression\"\n",
    "path_ckpts = f\"{path_root}/logs/Age\"\n",
    "pathlib.Path(path_ckpts).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "data_config = read_parse_config(f\"{path_configs}/DataConfig.yaml\", DataConfig)\n",
    "data_config['target'] = ['Age']\n",
    "data_config['continuous_cols'] = imms\n",
    "trainer_config = read_parse_config(f\"{path_configs}/TrainerConfig.yaml\", TrainerConfig)\n",
    "trainer_config['seed'] = seed\n",
    "trainer_config['checkpoints'] = 'valid_loss'\n",
    "trainer_config['load_best'] = True\n",
    "trainer_config['auto_lr_find'] = False\n",
    "trainer_config['checkpoints_path'] = path_ckpts\n",
    "optimizer_config = read_parse_config(f\"{path_configs}/OptimizerConfig.yaml\", OptimizerConfig)\n",
    "\n",
    "lr_find_min_lr = 1e-8\n",
    "lr_find_max_lr = 10\n",
    "lr_find_num_training = 512\n",
    "lr_find_mode = \"exponential\"\n",
    "lr_find_early_stop_threshold = 8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load default Models configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_archs = ['CategoryEmbeddingModel', 'DANet', 'FTTransformer', 'GANDALF', 'TabNetModel' ]\n",
    "configs_models_default = {}\n",
    "configs_models_default['DANet'] = read_parse_config(f\"{path_configs}/models/DANetConfig.yaml\", DANetConfig)\n",
    "configs_models_default['FTTransformer'] = read_parse_config(f\"{path_configs}/models/FTTransformerConfig.yaml\", FTTransformerConfig)\n",
    "configs_models_default['GANDALF'] = read_parse_config(f\"{path_configs}/models/GANDALFConfig.yaml\", GANDALFConfig)\n",
    "configs_models_default['TabNetModel'] = read_parse_config(f\"{path_configs}/models/TabNetModelConfig.yaml\", TabNetModelConfig)\n",
    "configs_models_default['CategoryEmbeddingModel'] = read_parse_config(f\"{path_configs}/models/CategoryEmbeddingModelConfig.yaml\", CategoryEmbeddingModelConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training immunomarkers models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPE sampler\n",
    "n_trials = 4 # 512\n",
    "opt_seed = seed\n",
    "n_startup_trials = 2 # 128\n",
    "n_ei_candidates = 1 # 16\n",
    "\n",
    "# Init optimization metrics with directions\n",
    "opt_parts = ['test', 'validation']\n",
    "opt_metrics = [('mean_absolute_error', 'minimize')]\n",
    "opt_directions = []\n",
    "for part in opt_parts:\n",
    "    for metric_pair in opt_metrics:\n",
    "        opt_directions.append(f\"{metric_pair[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation and Hyperparameter optimization training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes with results\n",
    "dfs_results = []\n",
    "\n",
    "# Loop for train-validation/test splits\n",
    "for split_id, split_dict in samples.items():\n",
    "    # Loop for train/validation folds\n",
    "    for fold_id in split_dict['trains']:\n",
    "        \n",
    "        test = df_controls.loc[split_dict['test'], imms + ['Age']]\n",
    "        train = df_controls.loc[split_dict['trains'][fold_id], imms + ['Age']]\n",
    "        validation = df_controls.loc[split_dict['validations'][fold_id], imms + ['Age']]\n",
    "        \n",
    "        # Loop for models archs\n",
    "        for m_arch in models_archs:\n",
    "            \n",
    "            tabular_model_default = TabularModel(\n",
    "                data_config=data_config,\n",
    "                model_config=configs_models_default[m_arch],\n",
    "                optimizer_config=optimizer_config,\n",
    "                trainer_config=trainer_config,\n",
    "                verbose=False,\n",
    "            )\n",
    "            datamodule = tabular_model_default.prepare_dataloader(train=train, validation=validation, seed=seed)\n",
    "            \n",
    "            trials_results = []\n",
    "            study = optuna.create_study(\n",
    "                study_name=f\"Age_{split_id}_{fold_id}_{m_arch}\",\n",
    "                sampler=optuna.samplers.TPESampler(\n",
    "                    n_startup_trials=n_startup_trials,\n",
    "                    n_ei_candidates=n_ei_candidates,\n",
    "                    seed=opt_seed,\n",
    "                ),\n",
    "                directions=opt_directions\n",
    "            )\n",
    "            study.optimize(\n",
    "                func=lambda trial: train_hyper_opt(\n",
    "                    trial=trial,\n",
    "                    trials_results=trials_results,\n",
    "                    opt_metrics=opt_metrics,\n",
    "                    opt_parts=opt_parts,\n",
    "                    model_config_default=configs_models_default[m_arch],\n",
    "                    data_config_default=data_config,\n",
    "                    optimizer_config_default=optimizer_config,\n",
    "                    trainer_config_default=trainer_config,\n",
    "                    experiment_config_default=None,\n",
    "                    train=train,\n",
    "                    validation=validation,\n",
    "                    test=test,\n",
    "                    datamodule=datamodule,\n",
    "                    min_lr=lr_find_min_lr,\n",
    "                    max_lr=lr_find_max_lr,\n",
    "                    num_training=lr_find_num_training,\n",
    "                    mode=lr_find_mode,\n",
    "                    early_stop_threshold=lr_find_early_stop_threshold\n",
    "                ), \n",
    "                n_trials=n_trials, \n",
    "                show_progress_bar=False\n",
    "            )\n",
    "            df_trials = pd.DataFrame(trials_results)\n",
    "            df_trials['split_id'] = split_id\n",
    "            df_trials['fold_id'] = fold_id\n",
    "            dfs_results.append(df_trials)\n",
    "\n",
    "# Resulting Dataframe\n",
    "df_results = pd.concat(dfs_results)            \n",
    "df_results.sort_values(by=['test_loss'], ascending=[True], inplace=True)\n",
    "df_results.to_excel(f\"{path_ckpts}/results.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
